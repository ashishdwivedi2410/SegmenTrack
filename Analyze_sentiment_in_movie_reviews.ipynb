{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "YAqhciN8V6wT",
        "sGxKzpcWslxs"
      ],
      "authorship_tag": "ABX9TyNzYOEmMGmfDyqqfrLL0JPB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashishdwivedi2410/SegmenTrack/blob/main/Analyze_sentiment_in_movie_reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Basic Text Processing**"
      ],
      "metadata": {
        "id": "YAqhciN8V6wT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Lowercasing the all text\n",
        "2. Remove Punctution\n",
        "3. Tokenization\n",
        "Text Split vs NLTK\n",
        "4. Removing Stoping words\n",
        "\n",
        "5. sremming\n",
        "*   use simple rules to chop off word ending.\n",
        "*   it does not check if the result is a void word\n",
        "*   A quick and dirty way of shortening words\n",
        "*   when speed matter more than accuracy\n",
        "\n",
        "6. Lemmatization\n",
        "*   it uses vocabulary and grammar ruloes to return real word\n",
        "*   a smarter more accurate version of stemming\n",
        "*   when understand and accuracy matter\n"
      ],
      "metadata": {
        "id": "0kVruAVSWF7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWjh4y5CkM0_",
        "outputId": "9ac0c124-d422-47b3-ed1d-6d76acc1604c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence.\"\n",
        "text.lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dU9LlbztWBN6",
        "outputId": "647d3406-a5e0-4091-e819-42828aa00b77"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing is an exciting field of artificial intelligence.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "punctuation = string.punctuation\n",
        "punctuation\n",
        "\n",
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence.\"\n",
        "\"\".join([char for char in text if char not in punctuation])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "47tkDlxbWBRY",
        "outputId": "888df151-354d-484f-9125-c48297fedbf1"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Natural Language Processing is an exciting field of Artificial Intelligence'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence.\"\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rl5XZ9E2WBfL",
        "outputId": "58ba48fd-4df1-4f25-b724-f5e42e5b27f8"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'is',\n",
              " 'an',\n",
              " 'exciting',\n",
              " 'field',\n",
              " 'of',\n",
              " 'Artificial',\n",
              " 'Intelligence.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('punkt_tab')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "text = \"I am Happy!\"\n",
        "word_tokenize(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jAetyh1WBmx",
        "outputId": "417389ab-e02a-49a4-f08c-ea1715091165"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'am', 'Happy', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import nltk\n",
        "#nltk.download('stopwords')\n",
        "\n",
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence.\"\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "tokens = word_tokenize(text.lower())\n",
        "\" \".join([word for word in tokens if word not in stopword])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ljGpB9kGWBqQ",
        "outputId": "72e5e8f6-e1f7-4579-d91a-1f029c4c1db3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natural language processing exciting field artificial intelligence .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "st = PorterStemmer()\n",
        "text = \"Natural Language Processing is an exciting field of Artificial Intelligence.\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\" \".join([st.stem(word) for word in tokens])"
      ],
      "metadata": {
        "id": "FZMzNhMhdzUZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "0146a3ba-e0da-47cc-9bca-d03c07b7334e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'natur languag process is an excit field of artifici intellig .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "\n",
        "lm.lemmatize(\"flies\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "15Z6Z4r0eECO",
        "outputId": "5b2f5025-ecf0-4e27-fd50-8af7131ab0bd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'fly'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installation Steps**"
      ],
      "metadata": {
        "id": "sGxKzpcWslxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow GPU Installation Guided using conda"
      ],
      "metadata": {
        "id": "kAMu54XIUYaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install bs4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQTNa-Y6UcrB",
        "outputId": "24985cd4-a98f-4095-c08e-f5a4c124becc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.15.0)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda cretate --name tensorflow_env python=3.8\n",
        "!conda activate tensorflow_env\n",
        "!conda install tensorflow-gpu"
      ],
      "metadata": {
        "id": "N0t9whofV6hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaee526a-29a4-439f-8ff3-8f22e6459172"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n",
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start Project**"
      ],
      "metadata": {
        "id": "ZEfTvLbbVibP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "import string\n"
      ],
      "metadata": {
        "id": "he1Gtrs_TEi5"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fADEkCjjTEmx",
        "outputId": "d0a79d85-6b17-4fba-f02d-41267d9a7f34"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD DATASET**"
      ],
      "metadata": {
        "id": "Y4xl6kldWgbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"/content/IMDB Dataset.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "X9w7nH7tTEqQ",
        "outputId": "939922c7-f2be-451d-92ff-2ece2ee1b1d0"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: EOF inside string starting at row 25310",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3673114524.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/IMDB Dataset.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 25310"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = data.dropna()\n",
        "df.shape"
      ],
      "metadata": {
        "id": "0OzdeA5fTE1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "UG3efvKNbE5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "punction = string.punctuation\n",
        "stopword = nltk.corpus.stopwords.words('english')\n",
        "stopword\n",
        "lm = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "yauqT2SIbL2B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = df['review'][1]\n",
        "text"
      ],
      "metadata": {
        "id": "9RQPQ2GocD5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BeautifulSoup(text, \"html.parser\").get_text()"
      ],
      "metadata": {
        "id": "cxvL0pQ1cjwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Stemming\n",
        "st = PorterStemmer()\n",
        "print(\"Stemming\")\n",
        "print(\"Playing ->\", st.stem(\"Playing\"))\n",
        "print(\"Played ->\", st.stem(\"Played\"))\n",
        "print(\"flies ->\", st.stem(\"flies\"))"
      ],
      "metadata": {
        "id": "MrD8MXnrPdcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Lemmatization\n",
        "lm = WordNetLemmatizer()\n",
        "print(\"Lemmatization\")\n",
        "print(\"Playing ->\", lm.lemmatize(\"Playing\"))\n",
        "print(\"Played ->\", lm.lemmatize(\"Played\"))\n",
        "print(\"flies ->\", lm.lemmatize(\"flies\"))"
      ],
      "metadata": {
        "id": "1wU6PuHsFErd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punctuation = string.punctuation\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "lm = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "0CssvcNtFE26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    text = \"\".join([char.lower() for char in text if char not in punctuation])\n",
        "    tokens = word_tokenize(text)\n",
        "    text = \" \".join([lm.lemmatize(char) for char in tokens if char not in stopwords])\n",
        "    return text"
      ],
      "metadata": {
        "id": "Cae_q2BUFE4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['review'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "dUrPhw7fFE7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y = df = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)"
      ],
      "metadata": {
        "id": "agUX49McFE94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "id": "OdHGKWRJFFBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Average num of words in text\n",
        "sum=0\n",
        "for i in X.to_list():\n",
        "    sum+=len(i.split(\" \"))\n",
        "avg = sum/len(X.to_list())\n",
        "avg\n"
      ],
      "metadata": {
        "id": "qY5Ce1YaH-zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Splitting dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.25)"
      ],
      "metadata": {
        "id": "JGiFCtfJH-18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,LSTM,Embedding,TextVectorization, Dropout"
      ],
      "metadata": {
        "id": "n3WMHwaGH-5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 20000\n",
        "max_len = 200\n",
        "vectorize_layer = TextVectorization(\n",
        " max_tokens=vocab_size,\n",
        " output_mode='int',\n",
        " output_sequence_length=max_len)"
      ],
      "metadata": {
        "id": "NBmB-9lsH-7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the vocabulary\n",
        "vectorize_layer.adapt(X_train)"
      ],
      "metadata": {
        "id": "kyFdpMpKH--H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = np.array(vectorize_layer.get_vocabulary())\n",
        "#First 50 vocabulary\n",
        "vocab[:50]"
      ],
      "metadata": {
        "id": "rCW3OLtMH_AU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1 = vectorize_layer(X_train)"
      ],
      "metadata": {
        "id": "3opo1QyxH_SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "hsl8ISMvH_VC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1"
      ],
      "metadata": {
        "id": "_2Kbx1oIH_Xs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "jIvc6MyZH_aN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Search indices of word\n",
        "foleys = np.where(vocab == 'noir')\n",
        "foleys"
      ],
      "metadata": {
        "id": "LQ89VuWkIRYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.to_list()[2]"
      ],
      "metadata": {
        "id": "93hvsLVuH_c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1[2,:]"
      ],
      "metadata": {
        "id": "lNYb2jhSIS_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "# (1,) (because we need to guarantee that there is exactly one string\n",
        "# input per batch), and the dtype needs to be 'string'.\n",
        "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
        "\n",
        "model.add(vectorize_layer)\n",
        "model.add(Embedding(vocab_size, 100))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1,activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "6GZKEUMGITBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.001),loss='binary_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Yxc00GQ-ITDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "early_stop = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history = model.fit(X_train,Y_train,validation_data=(X_test,Y_test),batch_size=32,epochs=10,\n",
        "                    callbacks = [early_stop])"
      ],
      "metadata": {
        "id": "lkMoJklJITHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Saving Model\n",
        "model.save(\"./Sentiment_Classifier_v1\")"
      ],
      "metadata": {
        "id": "evJ7IAbKITKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'],label='Training Acc')\n",
        "plt.plot(history.history['val_accuracy'],label='Test Acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AWd5MLyJITMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_text = 'The movie was cool. The animation and the graphics were out of this world. I would recommend this movie.'\n",
        "prediction = model.predict(np.array([Test_text]))\n",
        "if(prediction[0]>0.5):\n",
        "    print(\"Positive Sentiment\", \"\\nScore:\",prediction[0][0])\n",
        "else:\n",
        "    print(\"Negative Sentiment\",\"\\nScore:\",prediction[0][0])"
      ],
      "metadata": {
        "id": "tH-UXz4sITOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Test_text = 'The movie was good in first-half, but second half is worst, so i hate it'\n",
        "prediction = model.predict(np.array([Test_text]))\n",
        "if(prediction[0]>0.5):\n",
        "    print(\"Positive Sentiment\", \"\\nScore:\",prediction[0][0])\n",
        "else:\n",
        "    print(\"Negative Sentiment\",\"\\nScore:\",prediction[0][0])"
      ],
      "metadata": {
        "id": "HQgECob7ITTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(X_test)"
      ],
      "metadata": {
        "id": "sol0waoKITWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test.to_list())\n",
        "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
        "y_pred_binary"
      ],
      "metadata": {
        "id": "Tqfp3nGDImdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_binary.shape"
      ],
      "metadata": {
        "id": "cvv0GTHdIUMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = np.array([Y_test.to_list()]).T\n",
        "y_true.shape"
      ],
      "metadata": {
        "id": "ZtUQO0v_IUQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix(y_true,y_pred_binary))\n",
        "disp.plot()"
      ],
      "metadata": {
        "id": "0v2FNx26IwOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_true, y_pred_binary))"
      ],
      "metadata": {
        "id": "77n03JkBIwRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eRs11MSpI1-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "__Drv3nvI2CE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HIqRpW08I2E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QG__oB2mI2Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QecK6JJKI2KV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}